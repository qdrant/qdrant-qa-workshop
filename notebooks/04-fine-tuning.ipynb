{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Fine-tuning\n",
    "\n",
    "So far we haven't even trained a single model. We've only used some pretrained ones which should work well enough in general cases, but not necessarily in a specific domain with its own terminology. But that doesn't mean we need to start from scratch. Those original embeddings still capture some useful pieces of information, and we should rather **slightly adjust them**, instead of starting from the very beginning.\n",
    "\n",
    "![](images/fine_tuning.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T15:39:44.301424Z",
     "start_time": "2022-11-02T15:39:39.086203Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "from quaterion.dataset.similarity_samples import SimilarityPairSample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset should be represented as pairs of question and corresponding answer. If we knew there are several valid answers for a specific question, then we could divide it into groups. In our case, we'll assume there is a single answer for a given question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T15:39:44.309526Z",
     "start_time": "2022-11-02T15:39:44.304260Z"
    }
   },
   "outputs": [],
   "source": [
    "class TweetsQADataset(Dataset):\n",
    "\n",
    "    def __init__(self, subset: str = \"train\"):\n",
    "        self.dataset = pd.DataFrame(load_dataset(\"tweet_qa\")[subset])\n",
    "\n",
    "    def __getitem__(self, index) -> SimilarityPairSample:\n",
    "        item = self.dataset.iloc[index]\n",
    "        return SimilarityPairSample(\n",
    "            obj_a=item[\"Question\"],\n",
    "            obj_b=item[\"Tweet\"],\n",
    "            subgroup=hash(item[\"Tweet\"]),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset is ready, we need to start preparing the model. Since we want it to be importable in different notebooks, that has to be done in a separate file, `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T15:39:45.196087Z",
     "start_time": "2022-11-02T15:39:44.318672Z"
    }
   },
   "outputs": [],
   "source": [
    "from model import TweetsQAModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the training is almost identical to the one we do with PyTorch or PyTorch Lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T15:39:45.202110Z",
     "start_time": "2022-11-02T15:39:45.198372Z"
    }
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "from quaterion import Quaterion\n",
    "from quaterion.dataset import PairsSimilarityDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T15:39:52.323561Z",
     "start_time": "2022-11-02T15:39:45.204720Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = TweetsQADataset(\"test\")\n",
    "validation_dataset = TweetsQADataset(\"validation\")\n",
    "train_dataloader = PairsSimilarityDataLoader(train_dataset, batch_size=512)\n",
    "validation_dataloader = PairsSimilarityDataLoader(validation_dataset, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T15:39:53.027990Z",
     "start_time": "2022-11-02T15:39:52.325914Z"
    }
   },
   "outputs": [],
   "source": [
    "pl.seed_everything(42, workers=True)\n",
    "tweets_qa_model = TweetsQAModel(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T15:45:32.855599Z",
     "start_time": "2022-11-02T15:39:53.031422Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    min_epochs=1,\n",
    "    max_epochs=100,\n",
    "    auto_select_gpus=True,\n",
    "    num_sanity_val_steps=2,\n",
    ")\n",
    "Quaterion.fit(tweets_qa_model, trainer, train_dataloader, validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T15:45:33.453658Z",
     "start_time": "2022-11-02T15:45:32.862503Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_qa_model.save_servable(\"tweets_qa_servable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
